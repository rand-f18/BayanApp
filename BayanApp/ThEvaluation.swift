import SwiftUI
import AVFoundation
import Speech

struct ThEvaluationContentView: View {
    @StateObject private var audioRecorder = ThAudioRecorder()
    
    var body: some View {
        VStack {
            // Title at the very top
            Text("ØªÙ‚ÙŠÙŠÙ… Ù†Ø·Ù‚ Ø­Ø±Ù Ø§Ù„Ø«Ø§Ø¡")
                .font(.largeTitle)
                .fontWeight(.bold)
                .padding(.top, 50)
                .padding(.bottom, 50)

            // Instruction for the user
            Text("Ø­Ø§ÙˆÙ„ Ù†Ø·Ù‚ Ø­Ø±Ù Ø§Ù„Ø«Ø§Ø¡")
                .font(.title)
                .padding()
            
            Text(audioRecorder.feedbackMessage)
                .font(.headline)
                .padding()
            
            // Display the target letter "Ø«" in a larger font
            Text("Ø«")
                .font(.system(size: 300))
                .frame(width: 500, height: 500)
                .background(audioRecorder.isPassed ? Color.green : Color.gray)
                .cornerRadius(10)
                .foregroundColor(.white)
                .animation(.easeInOut(duration: 0.5), value: audioRecorder.isPassed)

            // Mic icon button
            Button(action: {
                if audioRecorder.isRecording {
                    audioRecorder.stopRecording()
                } else {
                    audioRecorder.startRecording()
                }
            }) {
                Image(systemName: "mic.fill")
                    .resizable()
                    .scaledToFit()
                    .frame(width: 75, height: 75)
                    .foregroundColor(audioRecorder.isRecording ? Color.red : Color.black)
                    .padding(.top, 100)
            }

            // User guidance
            Text("ØªØ£ÙƒØ¯ Ù…Ù† Ù†Ø·Ù‚ Ø­Ø±Ù Ø§Ù„Ø«Ø§Ø¡ Ø¨ÙˆØ¶ÙˆØ­.")
                .font(.subheadline)
                .foregroundColor(.secondary)
        }
        .padding()
    }
}

class ThAudioRecorder: NSObject, AVAudioRecorderDelegate, ObservableObject {
    @Published var feedbackMessage = ""
    @Published var isPassed = false
    @Published var isRecording = false
    let targetLetter = "Ø«" // ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø±Ù Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ø¥Ù„Ù‰ Ø«Ø§Ø¡
    private var audioRecorder: AVAudioRecorder?

    func startRecording() {
        isRecording = true
        feedbackMessage = "Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„..."

        configureAudioSession { success in
            if success {
                let audioFilename = self.getDocumentsDirectory().appendingPathComponent("recording.m4a")
                let settings = [
                    AVFormatIDKey: Int(kAudioFormatMPEG4AAC),
                    AVSampleRateKey: 12000,
                    AVNumberOfChannelsKey: 1,
                    AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue
                ]
                
                do {
                    self.audioRecorder = try AVAudioRecorder(url: audioFilename, settings: settings)
                    self.audioRecorder?.delegate = self
                    self.audioRecorder?.record()
                } catch {
                    self.feedbackMessage = "ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: \(error.localizedDescription)"
                    print("Error recording: \(error)")
                    self.isRecording = false
                }
            } else {
                self.feedbackMessage = "ÙØ´Ù„ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ù„Ø³Ø© Ø§Ù„ØµÙˆØª."
                self.isRecording = false
            }
        }
    }

    func stopRecording() {
        audioRecorder?.stop()
        isRecording = false
        feedbackMessage = "ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª."
        analyzeAudio()
    }

    func analyzeAudio() {
        let audioFilename = getDocumentsDirectory().appendingPathComponent("recording.m4a")
        print("Audio file path: \(audioFilename)")
        
        guard FileManager.default.fileExists(atPath: audioFilename.path) else {
            DispatchQueue.main.async {
                self.feedbackMessage = "ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„ØµÙˆØª."
            }
            return
        }

        SFSpeechRecognizer.requestAuthorization { authStatus in
            guard authStatus == .authorized else {
                DispatchQueue.main.async {
                    self.feedbackMessage = "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ù…Ù†Ø­ Ø§Ù„Ø¥Ø°Ù† Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª."
                }
                return
            }
            
            let arabicRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "ar-SA"))
            let request = SFSpeechURLRecognitionRequest(url: audioFilename)
            
            arabicRecognizer?.recognitionTask(with: request) { result, error in
                if let error = error {
                    DispatchQueue.main.async {
                        self.feedbackMessage = "ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª: \(error.localizedDescription)"
                        print("Recognition error: \(error)")
                    }
                    return
                }
                
                guard let result = result else {
                    DispatchQueue.main.async {
                        self.feedbackMessage = "ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª: Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªÙŠØ¬Ø©."
                    }
                    return
                }
                
                let spokenText = result.bestTranscription.formattedString
                print("Recognized text: \(spokenText)")
                
                DispatchQueue.main.async {
                    let cleanedText = spokenText
                        .replacingOccurrences(of: "\u{200E}", with: "")
                        .replacingOccurrences(of: "\u{200F}", with: "")
                        .trimmingCharacters(in: .whitespacesAndNewlines)
                    
                    print("Cleaned recognized text: '\(cleanedText)'")
                    print("Comparing '\(cleanedText)' with '\(self.targetLetter)'")
                    
                    // Set isPassed based on whether the target letter is the only recognized character
                    if cleanedText == self.targetLetter {
                        self.feedbackMessage = "Ù†Ø·Ù‚ ØµØ­ÙŠØ­! ðŸŽ‰" // Ø±Ø³Ø§Ù„Ø© Ø¹Ù†Ø¯ Ø§Ù„Ù†Ø·Ù‚ Ø§Ù„ØµØ­ÙŠØ­
                        self.isPassed = true // Ø£Ø®Ø¶Ø± Ù„Ù„Ù†Ø·Ù‚ Ø§Ù„ØµØ­ÙŠØ­
                        print("Matched! Background should turn green.")
                    } else {
                        self.feedbackMessage = "Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰." // Ø±Ø³Ø§Ù„Ø© Ø¹Ù†Ø¯ Ø§Ù„Ù†Ø·Ù‚ ØºÙŠØ± Ø§Ù„ØµØ­ÙŠØ­
                        self.isPassed = false // Ø±Ù…Ø§Ø¯ÙŠ Ù„Ù„Ù†Ø·Ù‚ ØºÙŠØ± Ø§Ù„ØµØ­ÙŠØ­
                        print("Did not match. Background should remain gray.")
                    }
                }
            }
        }
    }
    
    func configureAudioSession(completion: @escaping (Bool) -> Void) {
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.playAndRecord, mode: .default, options: .defaultToSpeaker)
            try audioSession.setActive(true)
            completion(true)
        } catch {
            print("Failed to set up audio session: \(error)")
            completion(false)
        }
    }
    
    func getDocumentsDirectory() -> URL {
        let paths = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)
        return paths[0]
    }
}

#Preview {
    ThEvaluationContentView()
}
